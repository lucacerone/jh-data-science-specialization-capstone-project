---
title: "Week 02 - Data Exploration"
author: "Luca Cerone"
output: 
html_document:
code_folding: hide
---

```{r setup}
suppressMessages({
  knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE, error = FALSE)
  library(readr)
  library(parallel)
  library(doParallel)
  library(iterators)
  library(tm)
  library(microbenchmark)
  
  clus <- makeCluster(detectCores()-1)
  registerDoParallel(clus)
  on.exit(stopCluster(clus))
})
```

## Importing the data

First of all I imported the three en_US datasets (blogs, twitter and news) into R.
Moreover I got from [github](https://github.com/dwyl/english-words) a list American English words (dictionary)
-- the repository has been released to the public domain, but the copyright belongs to [infochimps](http://www.infochimps.com)--.

```{r}
dictionary <- read_lines('../data/enUS_words.txt')
profanities <- read_lines('../data/en_US_swearWords.txt')
```

### Clean up the data

Before proceeding with counting the number of words I decided to clean up the data
according to the following rules (the filters might not be 100% accurate but should help remove
the majority of the unwanted cases):

- substitute URLs with one space
- substitute emails with one space
- substitute hashtags with one space
- substitute mentions with one space
- substitute all hypens and dashes with one space
- substitute all the non alpha numerical characters (this includes punctuation) with one space
- substitute all multiple spaces with one space
- remove all profanity words (I got the list of profanity words from http://www.bannedwordlist.com/)
- substitute all the words that do not belong to the dictionary I downloaded with 
the sequence <unknown>

```{r}
url_to_space <- function(x) { gsub("(http://|www).*?(\\s|$)", " ", x) }
email_to_space <- function(x) { gsub("[a-zA-Z0-9_+.\\-][@][.].+?(\\s+|$)", " ", x) }
hashtag_to_space <- function(x) { gsub("[#].*?(\\s|$)", " ", x) }
mention_to_space <- function(x) { gsub("[@].*?(\\s|$)", " ", x) }
hypen_to_space <- function(x) { gsub("[\\-_]", " ", x) }
nonalfanum_to_space <- function(x) { gsub("[^a-zA-Z]"," ", x) }
multiple_spaces_to_space <- function(x) { gsub("\\s+", " ", x) }
remove_spaces <- function(x) { gsub("\\s+", "", x) }
make_in_dictionary <- function(dictionary) {
  dictionary <- tolower(dictionary)
  function(x) {
    x %in% dictionary
  }
}

in_dictionary <- make_in_dictionary(dictionary)
is_profanity  <- make_in_dictionary(profanities)

remove_profanities <- function(x) {
  #words_list <- strsplit(x, " ")[[1]]
  x[is_profanity(x)] <- ""
  x
}

not_in_dictionary_to_unknown <- function(x) {
  #words_list <- strsplit(x, " ")[[1]]
  x[!in_dictionary(x)] <- "<unknown>"
  x
}

clean_data <- function(x) {
  x <- url_to_space(x)
  x <- email_to_space(x)
  x <- hashtag_to_space(x)
  x <- mention_to_space(x)
  x <- hypen_to_space(x)
  x <- nonalfanum_to_space(x)
  x <- remove_spaces(x)
  x <- tolower(x)
  x <- remove_profanities(x)
  not_in_dictionary_to_unknown(x)
}

clean_file <- function(orig, dest, batch_size = 10000) {
  scon <- file(orig, open = "rb", encoding = "UTF-8")
  on.exit(close(scon), add = T)
  
  unlink(dest)
  dcon <- file(dest, open = "a", encoding = "UTF-8")
  on.exit(close(dcon), add = T)
  x <- "OK"
  while ( length(x) > 0 ) {
    x <- readLines( scon, batch_size)
    if (length(x) == 0) break
    
    xl <- strsplit(x, split = " |[-]", perl = T)
    cl <- mclapply(xl, clean_data, mc.cores = 4, mc.cleanup = T)
    cs <- unlist(lapply(cl, paste, collapse = " "))
    
    if (!is.null(cs)) writeLines(cs, dcon)
    flush(dcon)
  }
  return(TRUE)
}
```

# Clean the samples for data exploration
```{r}
clean_file("../data/en_US/en_US.twitter.txt", "../data/en_US/en_US.twitter_cleaned.txt", 25000)
clean_file("../data/en_US/en_US.blogs.txt", "../data/en_US/en_US.blogs_cleaned.txt", 25000)
clean_file("../data/en_US/en_US.news.txt", "../data/en_US/en_US.news_cleaned.txt", 25000)
```

